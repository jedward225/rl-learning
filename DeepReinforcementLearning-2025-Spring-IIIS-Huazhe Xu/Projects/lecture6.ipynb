{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDW1wzlEkgNm"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn9bZAP3r4Be",
        "outputId": "2126538f-7de6-4a55-93bc-569d403d6398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: box2d\n",
            "Successfully installed box2d-2.3.10\n",
            "Collecting mediapy\n",
            "  Downloading mediapy-1.2.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapy) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapy) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mediapy) (11.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.17.0)\n",
            "Downloading mediapy-1.2.2-py3-none-any.whl (26 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, mediapy\n",
            "Successfully installed jedi-0.19.2 mediapy-1.2.2\n"
          ]
        }
      ],
      "source": [
        "%pip install gymnasium\n",
        "%pip install box2d\n",
        "%pip install mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJtNpQV1ktRc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Beta, Normal\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import argparse\n",
        "import mediapy as media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIck5t-TkWMr"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVY3Exg7kcHC"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, args):\n",
        "        self.s = np.zeros((args.batch_size, args.state_dim))\n",
        "        self.a = np.zeros((args.batch_size, args.action_dim))\n",
        "        self.a_logprob = np.zeros((args.batch_size, args.action_dim))\n",
        "        self.r = np.zeros((args.batch_size, 1))\n",
        "        self.s_ = np.zeros((args.batch_size, args.state_dim))\n",
        "        self.dw = np.zeros((args.batch_size, 1))\n",
        "        self.done = np.zeros((args.batch_size, 1))\n",
        "        self.count = 0\n",
        "\n",
        "    def store(self, s, a, a_logprob, r, s_, dw, done):\n",
        "        self.s[self.count] = s\n",
        "        self.a[self.count] = a\n",
        "        self.a_logprob[self.count] = a_logprob\n",
        "        self.r[self.count] = r\n",
        "        self.s_[self.count] = s_\n",
        "        self.dw[self.count] = dw\n",
        "        self.done[self.count] = done\n",
        "        self.count += 1\n",
        "\n",
        "    def numpy_to_tensor(self, device):\n",
        "        s = torch.tensor(self.s, dtype=torch.float, device=device)\n",
        "        a = torch.tensor(self.a, dtype=torch.float, device=device)\n",
        "        a_logprob = torch.tensor(self.a_logprob, dtype=torch.float, device=device)\n",
        "        r = torch.tensor(self.r, dtype=torch.float, device=device)\n",
        "        s_ = torch.tensor(self.s_, dtype=torch.float, device=device)\n",
        "        dw = torch.tensor(self.dw, dtype=torch.float, device=device)\n",
        "        done = torch.tensor(self.done, dtype=torch.float, device=device)\n",
        "\n",
        "        return s, a, a_logprob, r, s_, dw, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGYT0me_kvST"
      },
      "source": [
        "## Normalizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe8cyj2Jk2q3"
      },
      "outputs": [],
      "source": [
        "class RunningMeanStd:\n",
        "    # Dynamically calculate mean and std\n",
        "    def __init__(self, shape):  # shape:the dimension of input data\n",
        "        self.n = 0\n",
        "        self.mean = np.zeros(shape)\n",
        "        self.S = np.zeros(shape)\n",
        "        self.std = np.sqrt(self.S)\n",
        "\n",
        "    def update(self, x):\n",
        "        x = np.array(x)\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            self.mean = x\n",
        "            self.std = x\n",
        "        else:\n",
        "            old_mean = self.mean.copy()\n",
        "            self.mean = old_mean + (x - old_mean) / self.n\n",
        "            self.S = self.S + (x - old_mean) * (x - self.mean)\n",
        "            self.std = np.sqrt(self.S / self.n)\n",
        "\n",
        "\n",
        "class Normalization:\n",
        "    def __init__(self, shape):\n",
        "        self.running_ms = RunningMeanStd(shape=shape)\n",
        "\n",
        "    def __call__(self, x, update=True):\n",
        "        # Whether to update the mean and std,during the evaluating,update=False\n",
        "        if update:\n",
        "            self.running_ms.update(x)\n",
        "        x = (x - self.running_ms.mean) / (self.running_ms.std + 1e-8)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class RewardScaling:\n",
        "    def __init__(self, shape, gamma):\n",
        "        self.shape = shape  # reward shape=1\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.running_ms = RunningMeanStd(shape=self.shape)\n",
        "        self.R = np.zeros(self.shape)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.R = self.gamma * self.R + x\n",
        "        self.running_ms.update(self.R)\n",
        "        x = x / (self.running_ms.std + 1e-8)  # Only divided std\n",
        "        return x\n",
        "\n",
        "    def reset(self):  # When an episode is done,we should reset 'self.R'\n",
        "        self.R = np.zeros(self.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkJdPOtk85u"
      },
      "source": [
        "## Create PPO Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ALQEDrBlxGE"
      },
      "source": [
        "### Create Actor & Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNrgx5ymlYmn"
      },
      "outputs": [],
      "source": [
        "# Trick 8: orthogonal initialization\n",
        "def orthogonal_init(layer, gain=1.0):\n",
        "    nn.init.orthogonal_(layer.weight, gain=gain)\n",
        "    nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "\n",
        "class Actor_Beta(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Actor_Beta, self).__init__()\n",
        "        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)\n",
        "        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)\n",
        "        self.alpha_layer = nn.Linear(args.hidden_width, args.action_dim)\n",
        "        self.beta_layer = nn.Linear(args.hidden_width, args.action_dim)\n",
        "        self.activate_func = [nn.ReLU(), nn.Tanh()][args.use_tanh]  # Trick10: use tanh\n",
        "\n",
        "        if args.use_orthogonal_init:\n",
        "            print(\"------use_orthogonal_init------\")\n",
        "            orthogonal_init(self.fc1)\n",
        "            orthogonal_init(self.fc2)\n",
        "            orthogonal_init(self.alpha_layer, gain=0.01)\n",
        "            orthogonal_init(self.beta_layer, gain=0.01)\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = self.activate_func(self.fc1(s))\n",
        "        s = self.activate_func(self.fc2(s))\n",
        "        # alpha and beta need to be larger than 1,so we use 'softplus' as the activation function and then plus 1\n",
        "        alpha = F.softplus(self.alpha_layer(s)) + 1.0\n",
        "        beta = F.softplus(self.beta_layer(s)) + 1.0\n",
        "        return alpha, beta\n",
        "\n",
        "    def get_dist(self, s):\n",
        "        alpha, beta = self.forward(s)\n",
        "        dist = Beta(alpha, beta)\n",
        "        return dist\n",
        "\n",
        "    def mean(self, s):\n",
        "        alpha, beta = self.forward(s)\n",
        "        mean = alpha / (alpha + beta)  # The mean of the beta distribution\n",
        "        return mean\n",
        "\n",
        "\n",
        "class Actor_Gaussian(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Actor_Gaussian, self).__init__()\n",
        "        self.max_action = args.max_action\n",
        "        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)\n",
        "        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)\n",
        "        self.mean_layer = nn.Linear(args.hidden_width, args.action_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(1, args.action_dim))  # We use 'nn.Parameter' to train log_std automatically\n",
        "        self.activate_func = [nn.ReLU(), nn.Tanh()][args.use_tanh]  # Trick10: use tanh\n",
        "\n",
        "        if args.use_orthogonal_init:\n",
        "            print(\"------use_orthogonal_init------\")\n",
        "            orthogonal_init(self.fc1)\n",
        "            orthogonal_init(self.fc2)\n",
        "            orthogonal_init(self.mean_layer, gain=0.01)\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = self.activate_func(self.fc1(s))\n",
        "        s = self.activate_func(self.fc2(s))\n",
        "        mean = self.max_action * torch.tanh(self.mean_layer(s))  # [-1,1]->[-max_action,max_action]\n",
        "        return mean\n",
        "\n",
        "    def get_dist(self, s):\n",
        "        mean = self.forward(s)\n",
        "        log_std = self.log_std.expand_as(mean)  # To make 'log_std' have the same dimension as 'mean'\n",
        "        std = torch.exp(log_std)  # The reason we train the 'log_std' is to ensure std=exp(log_std)>0\n",
        "        dist = Normal(mean, std)  # Get the Gaussian distribution\n",
        "        return dist\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)\n",
        "        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)\n",
        "        self.fc3 = nn.Linear(args.hidden_width, 1)\n",
        "        self.activate_func = [nn.ReLU(), nn.Tanh()][args.use_tanh]  # Trick10: use tanh\n",
        "\n",
        "        if args.use_orthogonal_init:\n",
        "            print(\"------use_orthogonal_init------\")\n",
        "            orthogonal_init(self.fc1)\n",
        "            orthogonal_init(self.fc2)\n",
        "            orthogonal_init(self.fc3)\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = self.activate_func(self.fc1(s))\n",
        "        s = self.activate_func(self.fc2(s))\n",
        "        v_s = self.fc3(s)\n",
        "        return v_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktOFDq1bmBAm"
      },
      "source": [
        "### Create Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvH7COC_l-BT"
      },
      "outputs": [],
      "source": [
        "class PPO_continuous():\n",
        "    def __init__(self, args):\n",
        "        self.policy_dist = args.policy_dist\n",
        "        self.max_action = args.max_action\n",
        "        self.batch_size = args.batch_size\n",
        "        self.mini_batch_size = args.mini_batch_size\n",
        "        self.max_train_steps = args.max_train_steps\n",
        "        self.lr_a = args.lr_a  # Learning rate of actor\n",
        "        self.lr_c = args.lr_c  # Learning rate of critic\n",
        "        self.gamma = args.gamma  # Discount factor\n",
        "        self.lamda = args.lamda  # GAE parameter\n",
        "        self.epsilon = args.epsilon  # PPO clip parameter\n",
        "        self.K_epochs = args.K_epochs  # PPO parameter\n",
        "        self.entropy_coef = args.entropy_coef  # Entropy coefficient\n",
        "        self.set_adam_eps = args.set_adam_eps\n",
        "        self.use_grad_clip = args.use_grad_clip\n",
        "        self.use_lr_decay = args.use_lr_decay\n",
        "        self.use_adv_norm = args.use_adv_norm\n",
        "        self.device = torch.device(\"cpu\")\n",
        "\n",
        "        if self.policy_dist == \"Beta\":\n",
        "            self.actor = Actor_Beta(args).to(self.device)\n",
        "        else:\n",
        "            self.actor = Actor_Gaussian(args).to(self.device)\n",
        "        self.critic = Critic(args).to(self.device)\n",
        "\n",
        "        if self.set_adam_eps:  # Trick 9: set Adam epsilon=1e-5\n",
        "            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_a, eps=1e-5)\n",
        "            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.lr_c, eps=1e-5)\n",
        "        else:\n",
        "            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_a)\n",
        "            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.lr_c)\n",
        "\n",
        "    def evaluate(self, s):  # When evaluating the policy, we only use the mean\n",
        "        s = torch.unsqueeze(torch.tensor(s, dtype=torch.float, device=self.device), 0)\n",
        "        if self.policy_dist == \"Beta\":\n",
        "            a = self.actor.mean(s).detach().cpu().numpy().flatten()\n",
        "        else:\n",
        "            a = self.actor(s).detach().cpu().numpy().flatten()\n",
        "        return a\n",
        "\n",
        "    def choose_action(self, s):\n",
        "        s = torch.unsqueeze(torch.tensor(s, dtype=torch.float, device=self.device), 0)\n",
        "        if self.policy_dist == \"Beta\":\n",
        "            with torch.no_grad():\n",
        "                dist = self.actor.get_dist(s)\n",
        "                a = dist.sample()  # Sample the action according to the probability distribution\n",
        "                a_logprob = dist.log_prob(a)  # The log probability density of the action\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                dist = self.actor.get_dist(s)\n",
        "                a = dist.sample()  # Sample the action according to the probability distribution\n",
        "                a = torch.clamp(a, -self.max_action, self.max_action)  # [-max,max]\n",
        "                a_logprob = dist.log_prob(a)  # The log probability density of the action\n",
        "        return a.cpu().numpy().flatten(), a_logprob.cpu().numpy().flatten()\n",
        "\n",
        "    def update(self, replay_buffer, total_steps):\n",
        "        s, a, a_logprob, r, s_, dw, done = replay_buffer.numpy_to_tensor(device=self.device)  # Get training data\n",
        "        \"\"\"\n",
        "            Calculate the advantage using GAE\n",
        "            'dw=True' means dead or win, there is no next state s'\n",
        "            'done=True' represents the terminal of an episode(dead or win or reaching the max_episode_steps). When calculating the adv, if done=True, gae=0\n",
        "        \"\"\"\n",
        "        adv = []\n",
        "        gae = 0\n",
        "        with torch.no_grad():  # adv and v_target have no gradient\n",
        "            vs = self.critic(s)\n",
        "            vs_ = self.critic(s_)\n",
        "            deltas = r + self.gamma * (1.0 - dw) * vs_ - vs\n",
        "            for delta, d in zip(reversed(deltas.flatten().cpu().numpy()), reversed(done.flatten().cpu().numpy())):\n",
        "                gae = delta + self.gamma * self.lamda * gae * (1.0 - d)\n",
        "                adv.insert(0, gae)\n",
        "            adv = torch.tensor(adv, dtype=torch.float, device=self.device).view(-1, 1)\n",
        "            v_target = adv + vs\n",
        "            if self.use_adv_norm:  # Trick 1:advantage normalization\n",
        "                adv = ((adv - adv.mean()) / (adv.std() + 1e-5))\n",
        "\n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Random sampling and no repetition. 'False' indicates that training will continue even if the number of samples in the last time is less than mini_batch_size\n",
        "            for index in BatchSampler(SubsetRandomSampler(range(self.batch_size)), self.mini_batch_size, False):\n",
        "                dist_now = self.actor.get_dist(s[index])\n",
        "                dist_entropy = dist_now.entropy().sum(1, keepdim=True)  # shape(mini_batch_size X 1)\n",
        "                a_logprob_now = dist_now.log_prob(a[index])\n",
        "                # a/b=exp(log(a)-log(b))  In multi-dimensional continuous action space，we need to sum up the log_prob\n",
        "                ratios = torch.exp(a_logprob_now.sum(1, keepdim=True) - a_logprob[index].sum(1, keepdim=True))  # shape(mini_batch_size X 1)\n",
        "\n",
        "                surr1 = ratios * adv[index]  # Only calculate the gradient of 'a_logprob_now' in ratios\n",
        "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * adv[index]\n",
        "                actor_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy  # Trick 5: policy entropy\n",
        "                # Update actor\n",
        "                self.optimizer_actor.zero_grad()\n",
        "                actor_loss.mean().backward()\n",
        "                if self.use_grad_clip:  # Trick 7: Gradient clip\n",
        "                    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "                self.optimizer_actor.step()\n",
        "\n",
        "                v_s = self.critic(s[index])\n",
        "                critic_loss = F.mse_loss(v_target[index], v_s)\n",
        "                # Update critic\n",
        "                self.optimizer_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                if self.use_grad_clip:  # Trick 7: Gradient clip\n",
        "                    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "                self.optimizer_critic.step()\n",
        "\n",
        "        if self.use_lr_decay:  # Trick 6:learning rate Decay\n",
        "            self.lr_decay(total_steps)\n",
        "\n",
        "    def lr_decay(self, total_steps):\n",
        "        lr_a_now = self.lr_a * (1 - total_steps / self.max_train_steps)\n",
        "        lr_c_now = self.lr_c * (1 - total_steps / self.max_train_steps)\n",
        "        for p in self.optimizer_actor.param_groups:\n",
        "            p['lr'] = lr_a_now\n",
        "        for p in self.optimizer_critic.param_groups:\n",
        "            p['lr'] = lr_c_now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yowZ1xX7mIFw"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVFfTrCLp1wF"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub5g5dvxmKpY"
      },
      "outputs": [],
      "source": [
        "max_train_steps = 750000  #@param {type:\"integer\"}\n",
        "evaluate_freq = 5000\n",
        "policy_dist = \"Gaussian\"\n",
        "batch_size = 2048\n",
        "mini_batch_size = 64\n",
        "hidden_width = 64\n",
        "lr_a = 3e-4\n",
        "lr_c = 3e-4\n",
        "gamma = 0.99\n",
        "lamda = 0.95\n",
        "epsilon = 0.2\n",
        "K_epochs = 10\n",
        "use_adv_norm = True        #@param {type:\"boolean\"}\n",
        "use_state_norm = True      #@param {type:\"boolean\"}\n",
        "use_reward_norm = False\n",
        "use_reward_scaling = True  #@param {type:\"boolean\"}\n",
        "entropy_coef = 0.01\n",
        "use_lr_decay = True\n",
        "use_grad_clip = True       #@param {type:\"boolean\"}\n",
        "use_orthogonal_init = True\n",
        "set_adam_eps = 1e-5\n",
        "use_tanh = True\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    max_train_steps=max_train_steps,\n",
        "    evaluate_freq=evaluate_freq,\n",
        "    policy_dist=policy_dist,\n",
        "    batch_size=batch_size,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    hidden_width=hidden_width,\n",
        "    lr_a=lr_a,\n",
        "    lr_c=lr_c,\n",
        "    gamma=gamma,\n",
        "    lamda=lamda,\n",
        "    epsilon=epsilon,\n",
        "    K_epochs=K_epochs,\n",
        "    use_adv_norm=use_adv_norm,\n",
        "    use_state_norm=use_state_norm,\n",
        "    use_reward_norm=use_reward_norm,\n",
        "    use_reward_scaling=use_reward_scaling,\n",
        "    entropy_coef=entropy_coef,\n",
        "    use_lr_decay=use_lr_decay,\n",
        "    use_grad_clip=use_grad_clip,\n",
        "    use_orthogonal_init=use_orthogonal_init,\n",
        "    set_adam_eps=set_adam_eps,\n",
        "    use_tanh=use_tanh\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuFi3XE7qXKo"
      },
      "source": [
        "### Training & Evaluating Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZtIK9yhqXwu"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(args, env, agent, state_norm, seed, evaluation_num):\n",
        "    times = 3\n",
        "    evaluate_reward = 0\n",
        "    for _ in range(times):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        if args.use_state_norm:\n",
        "            s = state_norm(s, update=False)  # During the evaluating,update=False\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            a = agent.evaluate(s)  # We use the deterministic policy during the evaluating\n",
        "            if args.policy_dist == \"Beta\":\n",
        "                action = 2 * (a - 0.5) * args.max_action  # [0,1]->[-max,max]\n",
        "            else:\n",
        "                action = a\n",
        "            s_, r, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if args.use_state_norm:\n",
        "                s_ = state_norm(s_, update=False)\n",
        "            episode_reward += r\n",
        "            s = s_\n",
        "        evaluate_reward += episode_reward\n",
        "\n",
        "    if evaluation_num % 20 == 0:\n",
        "        done = False\n",
        "        s, _ = env.reset(seed=seed)\n",
        "\n",
        "        if args.use_state_norm:\n",
        "            s = state_norm(s, update=False)\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            a = agent.evaluate(s)  # We use the deterministic policy during the evaluating\n",
        "            if args.policy_dist == \"Beta\":\n",
        "                action = 2 * (a - 0.5) * args\n",
        "            else:\n",
        "                action = a\n",
        "            s_, r, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if args.use_state_norm:\n",
        "                s_ = state_norm(s_, update=False)\n",
        "            episode_reward += r\n",
        "            s = s_\n",
        "\n",
        "    return evaluate_reward / times\n",
        "\n",
        "\n",
        "def main(args, env_name, seed):\n",
        "    env = gym.make(env_name)\n",
        "    env_evaluate = gym.make(env_name)  # When evaluating the policy, we need to rebuild an environment\n",
        "    # Set random seed\n",
        "    env.action_space.seed(seed)\n",
        "    env_evaluate.action_space.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    args.state_dim = env.observation_space.shape[0]\n",
        "    args.action_dim = env.action_space.shape[0]\n",
        "    args.max_action = float(env.action_space.high[0])\n",
        "    args.max_episode_steps = env._max_episode_steps  # Maximum number of steps per episode\n",
        "    print(\"env={}\".format(env_name))\n",
        "    print(\"state_dim={}\".format(args.state_dim))\n",
        "    print(\"action_dim={}\".format(args.action_dim))\n",
        "    print(\"max_action={}\".format(args.max_action))\n",
        "    print(\"max_episode_steps={}\".format(args.max_episode_steps))\n",
        "\n",
        "    evaluate_num = 0  # Record the number of evaluations\n",
        "    evaluate_rewards = []  # Record the rewards during the evaluating\n",
        "    total_steps = 0  # Record the total steps during the training\n",
        "\n",
        "    replay_buffer = ReplayBuffer(args)\n",
        "    agent = PPO_continuous(args)\n",
        "\n",
        "    # Build a tensorboard\n",
        "    writer = SummaryWriter(\n",
        "        log_dir='runs/PPO_continuous/{}_adv_norm={}_state_norm={}_reward_scaling={}_use_grad_clip={}_seed={}'.format(\n",
        "            env_name, args.use_adv_norm, args.use_state_norm, args.use_reward_scaling, args.use_grad_clip, seed\n",
        "    ))\n",
        "\n",
        "    state_norm = Normalization(shape=args.state_dim)  # Trick 2:state normalization\n",
        "    if args.use_reward_norm:  # Trick 3:reward normalization\n",
        "        reward_norm = Normalization(shape=1)\n",
        "    elif args.use_reward_scaling:  # Trick 4:reward scaling\n",
        "        reward_scaling = RewardScaling(shape=1, gamma=args.gamma)\n",
        "\n",
        "    while total_steps < args.max_train_steps:\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        if args.use_state_norm:\n",
        "            s = state_norm(s)\n",
        "        if args.use_reward_scaling:\n",
        "            reward_scaling.reset()\n",
        "        episode_steps = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            episode_steps += 1\n",
        "            a, a_logprob = agent.choose_action(s)  # Action and the corresponding log probability\n",
        "            if args.policy_dist == \"Beta\":\n",
        "                action = 2 * (a - 0.5) * args.max_action  # [0,1]->[-max,max]\n",
        "            else:\n",
        "                action = a\n",
        "            s_, r, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if args.use_state_norm:\n",
        "                s_ = state_norm(s_)\n",
        "            if args.use_reward_norm:\n",
        "                r = reward_norm(r)\n",
        "            elif args.use_reward_scaling:\n",
        "                r = reward_scaling(r)\n",
        "\n",
        "            # When dead or win or reaching the max_episode_steps, done will be Ture, we need to distinguish them;\n",
        "            # dw means dead or win,there is no next state s';\n",
        "            # but when reaching the max_episode_steps,there is a next state s' actually.\n",
        "            if done and episode_steps != args.max_episode_steps:\n",
        "                dw = True\n",
        "            else:\n",
        "                dw = False\n",
        "\n",
        "            # Take the 'action'，but store the original 'a'（especially for Beta）\n",
        "            replay_buffer.store(s, a, a_logprob, r, s_, dw, done)\n",
        "            s = s_\n",
        "            total_steps += 1\n",
        "\n",
        "            # When the number of transitions in buffer reaches batch_size,then update\n",
        "            if replay_buffer.count == args.batch_size:\n",
        "                agent.update(replay_buffer, total_steps)\n",
        "                replay_buffer.count = 0\n",
        "\n",
        "            # Evaluate the policy every 'evaluate_freq' steps\n",
        "            if total_steps % args.evaluate_freq == 0:\n",
        "                evaluate_num += 1\n",
        "                evaluate_reward = evaluate_policy(args, env_evaluate, agent, state_norm, seed, evaluation_num=evaluate_num)\n",
        "                evaluate_rewards.append(evaluate_reward)\n",
        "                print(\"evaluate_num:{} \\t evaluate_reward:{} \\t\".format(evaluate_num, evaluate_reward))\n",
        "                writer.add_scalar('step_rewards_{}'.format(env_name), evaluate_rewards[-1], global_step=total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VipGdlr4rVrO"
      },
      "source": [
        "## Start Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdV_3Z4rPom",
        "outputId": "02ec4776-2924-404b-eef4-6f143b711542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env=BipedalWalker-v3\n",
            "state_dim=24\n",
            "action_dim=4\n",
            "max_action=1.0\n",
            "max_episode_steps=1600\n",
            "------use_orthogonal_init------\n",
            "------use_orthogonal_init------\n",
            "evaluate_num:1 \t evaluate_reward:-107.3402328491211 \t\n",
            "evaluate_num:2 \t evaluate_reward:-96.1791000366211 \t\n",
            "evaluate_num:3 \t evaluate_reward:-17.41778564453125 \t\n",
            "evaluate_num:4 \t evaluate_reward:-115.8319091796875 \t\n",
            "evaluate_num:5 \t evaluate_reward:-34.95669174194336 \t\n",
            "evaluate_num:6 \t evaluate_reward:-40.71194839477539 \t\n",
            "evaluate_num:7 \t evaluate_reward:-47.07362747192383 \t\n",
            "evaluate_num:8 \t evaluate_reward:-48.43290710449219 \t\n",
            "evaluate_num:9 \t evaluate_reward:-39.24855041503906 \t\n",
            "evaluate_num:10 \t evaluate_reward:-47.48784255981445 \t\n",
            "evaluate_num:11 \t evaluate_reward:-21.88114356994629 \t\n",
            "evaluate_num:12 \t evaluate_reward:-5.404932022094727 \t\n",
            "evaluate_num:13 \t evaluate_reward:-4.766114234924316 \t\n",
            "evaluate_num:14 \t evaluate_reward:-7.458475589752197 \t\n",
            "evaluate_num:15 \t evaluate_reward:-4.645059585571289 \t\n",
            "evaluate_num:16 \t evaluate_reward:5.089202404022217 \t\n",
            "evaluate_num:17 \t evaluate_reward:8.3074951171875 \t\n",
            "evaluate_num:18 \t evaluate_reward:4.642306804656982 \t\n",
            "evaluate_num:19 \t evaluate_reward:18.228145599365234 \t\n",
            "evaluate_num:20 \t evaluate_reward:28.53557014465332 \t\n",
            "evaluate_num:21 \t evaluate_reward:54.26258850097656 \t\n"
          ]
        }
      ],
      "source": [
        "env_name = \"BipedalWalker-v3\"\n",
        "main(args, env_name=env_name, seed=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO4WneoO0_S7"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aIck5t-TkWMr",
        "rGYT0me_kvST",
        "aXkJdPOtk85u",
        "0ALQEDrBlxGE",
        "ktOFDq1bmBAm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}